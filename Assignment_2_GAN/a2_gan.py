# -*- coding: utf-8 -*-
"""A2_GAN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SX_4R9IgU8CtSdCT93tIlAs450dzbSyN
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

rollNumber = 102317167
ar = 0.5 * (rollNumber % 7)
br = 0.3 * (rollNumber % 5 + 1)

print(ar)
print(br)

dataFile = pd.read_csv('data.csv', encoding = 'latin1')
no2Values = dataFile['no2'].dropna().values
xData = no2Values[~np.isnan(no2Values)]

zTransformed = xData + ar * np.sin(br * xData)

zMean = np.mean(zTransformed)
zStd = np.std(zTransformed)
zNormalized = (zTransformed - zMean) / zStd

class SimpleGenerator(nn.Module):
    def __init__(self):
        super(SimpleGenerator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)

class SimpleDiscriminator(nn.Module):
    def __init__(self):
        super(SimpleDiscriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

genModel = SimpleGenerator()
discModel = SimpleDiscriminator()

optimG = optim.Adam(genModel.parameters(), lr=0.002)
optimD = optim.Adam(discModel.parameters(), lr=0.0005)

criterion = nn.BCELoss()

bSize = 256
nEpochs = 50

gLosses = []
dLosses = []

realTensor = torch.FloatTensor(zNormalized.reshape(-1, 1))
numSamples = len(realTensor)

for ep in range(nEpochs):
    indices = torch.randperm(numSamples)

    epochDLoss = 0
    epochGLoss = 0
    nBatch = 0

    for i in range(0, numSamples, bSize):
        batchIdx = indices[i:i+bSize]
        realBatch = realTensor[batchIdx]
        currBSize = realBatch.size(0)

        optimD.zero_grad()

        realOut = discModel(realBatch)
        realLoss = criterion(realOut, torch.ones(currBSize, 1))

        noise = torch.randn(currBSize, 10)
        fakeData = genModel(noise)
        fakeOut = discModel(fakeData.detach())
        fakeLoss = criterion(fakeOut, torch.zeros(currBSize, 1))

        dLoss = realLoss + fakeLoss
        dLoss.backward()
        optimD.step()

        optimG.zero_grad()

        noise = torch.randn(currBSize, 10)
        fakeData = genModel(noise)
        fakeOut = discModel(fakeData)
        gLoss = criterion(fakeOut, torch.ones(currBSize, 1))

        gLoss.backward()
        optimG.step()

        epochDLoss += dLoss.item()
        epochGLoss += gLoss.item()
        nBatch += 1

    avgD = epochDLoss / nBatch
    avgG = epochGLoss / nBatch

    gLosses.append(avgG)
    dLosses.append(avgD)

    if ep % 10 == 0:
        print(f"Epoch {ep}/{nEpochs} - D Loss: {avgD:.4f}, G Loss: {avgG:.4f}")

print("Training completed!")

numGenSamples = 50000
noiseGen = torch.randn(numGenSamples, 10)
with torch.no_grad():
    genSamples = genModel(noiseGen).numpy()

genSamples = genSamples * zStd + zMean

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(zTransformed, bins=80, density=True, alpha=0.6, color='blue', label='Real Data')
plt.hist(genSamples, bins=80, density=True, alpha=0.6, color='red', label='Generated Data')
plt.xlabel('z values')
plt.ylabel('Density')
plt.title('Histogram Comparison')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
kdeReal = gaussian_kde(zTransformed)
kdeGen = gaussian_kde(genSamples.flatten())
minVal = min(zTransformed.min(), genSamples.min())
maxVal = max(zTransformed.max(), genSamples.max())
zRange = np.linspace(minVal, maxVal, 500)
plt.plot(zRange, kdeReal(zRange), label='Real PDF', color='blue', linewidth=2)
plt.plot(zRange, kdeGen(zRange), label='Generated PDF', color='red', linewidth=2)
plt.xlabel('z values')
plt.ylabel('Probability Density')
plt.title('KDE Estimated PDF')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
plt.plot(gLosses, label='Generator', color='orange', linewidth=1.5)
plt.plot(dLosses, label='Discriminator', color='green', linewidth=1.5)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('gan_pdf_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*50)
print("RESULTS SUMMARY")
print("="*50)
print(f"Transformation: ar = {ar}, br = {br}")
print(f"\nReal Data Statistics:")
print(f"  Mean: {np.mean(zTransformed):.4f}")
print(f"  Std:  {np.std(zTransformed):.4f}")
print(f"  Min:  {np.min(zTransformed):.4f}")
print(f"  Max:  {np.max(zTransformed):.4f}")
print(f"\nGenerated Data Statistics:")
print(f"  Mean: {np.mean(genSamples):.4f}")
print(f"  Std:  {np.std(genSamples):.4f}")
print(f"  Min:  {np.min(genSamples):.4f}")
print(f"  Max:  {np.max(genSamples):.4f}")
print("\nFinal Training Losses:")
print(f"  Generator: {gLosses[-1]:.4f}")
print(f"  Discriminator: {dLosses[-1]:.4f}")
print("="*50)

